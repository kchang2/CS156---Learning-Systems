{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 8\n",
    "## CS156, Kai Chang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Answer Choice: **D**, a quadratic programming problem with $d+1$ variables<br>\n",
    "Reasoning: Given $d$ represents the diemsnionality of the space, we know that for the primal problem we want to minimize $\\frac{1}{2}w^Tw$ such that for all $i, y_i(w^Tx_i + w_0) \\geq 1$.\n",
    "\n",
    "And because the weight is the variable we are minimizing, thus this can be solved using a quadratic program with $d+1$ variables, and an $i$ constraints. Note that it is $d+1$ instead of $d$ because we also have to consider the w_0 term (ie. the bias). Also note for the dual problem, we look at the size of the data set $N$, rather than the dimensionality of the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Answer Choice: **A**, 0 versus all<br>\n",
    "Reasoning: see code. Note we are using E_in, not E_out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Used packages from: http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
    "Data is structured in text file as : digit intensity symmetry\n",
    "Data will be structured in program as: [[digit, intensity, symmetry]]\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "t_d = np.loadtxt('features.train')\n",
    "x_train = t_d[:,1:]\n",
    "y_train = t_d[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def poly_kernel(x_n, x_m, Q):\n",
    "    '''\n",
    "    Polynomial kernal used in the soft margin SVM. Format expanded looks like this\n",
    "    K(x, x') = (1 + x^T . x')^Q\n",
    "             = (1 + x1x'1 + x2x'2 + ... + xdx'd)^Q\n",
    "    where d represents the number of features per data point\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_n : first data point, numpy array\n",
    "    x_m : second data point, numpy array\n",
    "    Q   : degree of polynomial, int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    equivalent kernel\n",
    "    '''\n",
    "    \n",
    "    return (1.0 + np.dot(x_n.T, x_m))**Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bin_class(y, n):\n",
    "    '''\n",
    "    Applies the one-versus-all binary classifier. Can be used for the \n",
    "    one-versus-one binary classifier through a computational trick.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : dataset output, numpy array\n",
    "    n : one digit value to classify as +1\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy array of binary classified values\n",
    "    '''\n",
    "    y_bin = -np.ones(len(y))\n",
    "    y_bin[y == n] = 1\n",
    "    return y_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_in for 0 versus all:  0.105883966534\n",
      "E_in for 2 versus all:  0.100260595254\n",
      "E_in for 4 versus all:  0.0894253188863\n",
      "E_in for 6 versus all:  0.0910711836511\n",
      "E_in for 8 versus all:  0.0743382252092\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "'''\n",
    "The kernel function can be any of the following:\n",
    "    linear: <x, x'>\n",
    "    polynomial: (γ<x, x'> + r)^d, γ = gamma, d = degree, r = coef0\n",
    "    rbf: exp(-γ|x-x'|^2). γ = gamma, must be > 0\n",
    "    sigmoid (tanh(γ<x,x'> + r)), γ = gamma, r = coef0\n",
    "\n",
    "From directly the link: http://scikit-learn.org/stable/modules/svm.html.\n",
    "Note < > suggests dot product with transpose, ie. <x, x'> = x^T . x'\n",
    "'''\n",
    "# parameters for kernel, SVM\n",
    "C = 0.01\n",
    "Q = 2\n",
    "\n",
    "# svm, with desired properties (attributes)\n",
    "clf = svm.SVC(C=C, kernel='poly', degree=Q, gamma=1, coef0=1)\n",
    "\n",
    "for n in [0, 2, 4, 6, 8]:\n",
    "    # apply binary classification\n",
    "    y_t_b = bin_class(y_train, n)\n",
    "    clf.fit(x_train, y_t_b)\n",
    "    y_t_b_ = clf.predict(x_train)\n",
    "    E_in = np.sum(abs(y_t_b_ + y_t_b) == 0) / float(len(y_t_)) # numpy sum has boolean test implementation\n",
    "    print 'E_in for %i versus all: ' %n, E_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Answer Choice: **A**, 1 versus all<br>\n",
    "Reasoning: see code result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_in for 1 versus all:  0.0144013166918\n",
      "E_in for 3 versus all:  0.0902482512687\n",
      "E_in for 5 versus all:  0.0762584007681\n",
      "E_in for 7 versus all:  0.0884652311068\n",
      "E_in for 9 versus all:  0.0883280757098\n"
     ]
    }
   ],
   "source": [
    "# parameters for kernel, SVM\n",
    "C = 0.01\n",
    "Q = 2\n",
    "\n",
    "# svm, with desired properties (attributes)\n",
    "clf = svm.SVC(C=C, kernel='poly', degree=Q, gamma=1, coef0=1)\n",
    "\n",
    "for n in [1, 3, 5, 7, 9]:\n",
    "    # apply binary classification\n",
    "    y_t_b = bin_class(y_train, n)\n",
    "    clf.fit(x_train, y_t_b)\n",
    "    y_t_b_ = clf.predict(x_train)\n",
    "    E_in = np.sum(abs(y_t_b_ + y_t_b) == 0) / float(len(y_t_)) # numpy sum has boolean test implementation\n",
    "    print 'E_in for %i versus all: ' %n, E_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Answer Choice: **C**, 1800 <br>\n",
    "Reasoning: see code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average support classifer vectors for 0 versus all case:  2179\n",
      "Average support classifer vectors for 1 versus all case:  386\n"
     ]
    }
   ],
   "source": [
    "# parameters for kernel, SVM\n",
    "C = 0.01\n",
    "Q = 2\n",
    "\n",
    "# svm, with desired properties (attributes)\n",
    "clf = svm.SVC(C=C, kernel='poly', degree=Q, gamma=1, coef0=1)\n",
    "\n",
    "for n in [0, 1]:\n",
    "    # apply binary classification\n",
    "    y_t_b_temp = bin_class(y_train, n)\n",
    "    clf.fit(x_train, y_t_b_temp)\n",
    "    y_t_b_temp_ = clf.predict(x_train)\n",
    "    E_in = np.sum(abs(y_t_b_temp + y_t_b_temp_) == 0) / float(len(y_t_b_temp_)) # numpy sum has boolean test implementation\n",
    "    print 'Average support classifer vectors for %i versus all case: ' %n, len(clf.support_vectors_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference between the number of support vectors of these two classifiers:  1793\n"
     ]
    }
   ],
   "source": [
    "print 'difference between the number of support vectors of these two classifiers: ', 2179 - 386"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Answer Choice: **D**, Maximum $C$ achieves the lowest $E_{in}$<br>\n",
    "Reasoning: see code. This is using the one-versus-one classification! I originally was going to select the data using y_train == 1, and then y_train == 5 and join the two, but numpy has a neat function that handles this case. Numpy is truly powerful. We also should realize that because the other data points do not matter, we remove them from your SVM learning process. If we do not remove them, we will note that the learning will suck.\n",
    "\n",
    "\n",
    "We see from the result that $E_{in}$ does not strictly decrease, $E_{out}$ does not strictly increase, so *C* is invalid. We also see that the support vector decreases, but not strictly, so *A* and *B* are ruled out. Now, we do see that for $C=1$ or the maximum $C$ in this $C$-set, it has the lowest $E_{in}$. Thus, *D* is our solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C = 0.001000,  Support vectors:  76 E_in:  0.00448430493274 E_out:  0.0165094339623\n",
      "For C = 0.010000,  Support vectors:  34 E_in:  0.00448430493274 E_out:  0.0188679245283\n",
      "For C = 0.100000,  Support vectors:  24 E_in:  0.00448430493274 E_out:  0.0188679245283\n",
      "For C = 1.000000,  Support vectors:  24 E_in:  0.00320307495195 E_out:  0.0188679245283\n"
     ]
    }
   ],
   "source": [
    "# load test dataset\n",
    "t_d = np.loadtxt('features.test')\n",
    "x_test = t_d[:,1:]\n",
    "y_test = t_d[:,0]\n",
    "\n",
    "# parameters for kernel, SVM\n",
    "C_list = [0.001, 0.01, 0.1, 1]\n",
    "Q = 2\n",
    "\n",
    "x_train_ = x_train[np.logical_or(y_train==1, y_train==5), :]\n",
    "y_t_b = bin_class(y_train[np.logical_or(y_train==1, y_train==5)], 1)\n",
    "# x_train[np.logical_or(y_train==1, y_train==5), :].shape\n",
    "x_test_ = x_test[np.logical_or(y_test==1, y_test==5), :]\n",
    "y_test_b = bin_class(y_test[np.logical_or(y_test==1, y_test==5)], 1)\n",
    "\n",
    "'''\n",
    "Note we could also make this easier by setting select_process = np.logical_or(y_train==1, y_train==5)\n",
    "Thus, we then do x_train_ = x_train[select_process, :]\n",
    "                 y_train_ = bin_class(y_train[select_process], 1)\n",
    "'''\n",
    "\n",
    "for C in C_list:\n",
    "    clf = svm.SVC(C=C, kernel='poly', degree=Q, gamma=1, coef0=1)\n",
    "    clf.fit(x_train_, y_t_b)\n",
    "    \n",
    "    y_t_b_ = clf.predict(x_train_)\n",
    "    y_test_b_ = clf.predict(x_test_)\n",
    "    \n",
    "    E_in = np.sum(abs(y_t_b_ + y_t_b) == 0) / float(len(y_t_b_))\n",
    "    E_out = np.sum(abs(y_test_b_ + y_test_b) == 0) / float(len(y_test_b_))\n",
    "    print 'For C = %f, ' %C, 'Support vectors: ', len(clf.support_vectors_), 'E_in: ', E_in, 'E_out: ', E_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Answer Choice: **B**, When $C=0.001$, the number of support vectors is lower at $Q=5$<br>\n",
    "Reasoning: see code. We are doing a comparision between the two $Q$'s so $C$ has the outer loop. We are still studying a 1 versus 5 classifier. Note that the computational time slows down (increases) with each step forward in C_list; just an interesting thought, but I'm not entirely sure why.\n",
    "\n",
    "We see $E_{in}$ is larger for $Q=2$ at $C = 0.0001$ and $0.01$, ruling out choices *A* and *C*. We also see $E_{out}$ is larger for $Q=5$ at $C=1$, ruling out *D*. At $C=0.001$, there are more support vectors for $Q=2$, meaning the number of support vectors is lower at $Q=5$. So, our answer is *B*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structure format: [Q=2, Q=5]\n",
      "For C = 0.000100 E_in:  ['0.008969', '0.004484'] E_out:  ['0.016509', '0.018868'] Support vectors:  [236, 26]\n",
      "For C = 0.001000 E_in:  ['0.004484', '0.004484'] E_out:  ['0.016509', '0.021226'] Support vectors:  [76, 25]\n",
      "For C = 0.010000 E_in:  ['0.004484', '0.003844'] E_out:  ['0.018868', '0.021226'] Support vectors:  [34, 23]\n",
      "For C = 1.000000 E_in:  ['0.003203', '0.003203'] E_out:  ['0.018868', '0.021226'] Support vectors:  [24, 21]\n"
     ]
    }
   ],
   "source": [
    "# parameters for kernel, SVM\n",
    "C_list = [0.0001, 0.001, 0.01, 1]\n",
    "Q_list = [2, 5]\n",
    "\n",
    "print 'structure format: [Q=2, Q=5]'\n",
    "for C in C_list:\n",
    "    Q_E_in = []\n",
    "    Q_E_out = []\n",
    "    Q_sv = []\n",
    "    for Q in Q_list:\n",
    "        clf = svm.SVC(C=C, kernel='poly', degree=Q, gamma=1, coef0=1)\n",
    "        clf.fit(x_train_, y_t_b)\n",
    "    \n",
    "        y_t_b_ = clf.predict(x_train_)\n",
    "        y_test_b_ = clf.predict(x_test_)\n",
    "        \n",
    "        Q_E_in.append(np.sum(abs(y_t_b_ + y_t_b) == 0) / float(len(y_t_b_)))\n",
    "        Q_E_out.append(np.sum(abs(y_test_b_ + y_test_b) == 0) / float(len(y_test_b_)))\n",
    "        Q_sv.append(len(clf.support_vectors_))\n",
    "        \n",
    "    print 'For C = %f' %C, 'E_in: ', [ '%.6f' % elem for elem in Q_E_in], \\\n",
    "          'E_out: ', [ '%.6f' % elem for elem in Q_E_out], 'Support vectors: ', Q_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "Answer Choice: **B**, $C=0.001$ is selected most often.<br>\n",
    "Reasoning: see code. I had a ton of trouble with importing as such:\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "and I believe it was due to my scikit-learn version. I couldn't get the version to update properly, so I stuck with scikit-learn 0.18, and worked with the older version of sklearn using cross_validation, found from \n",
    "http://stackoverflow.com/questions/16379313/how-to-use-the-a-k-fold-cross-validation-in-scikit-with-naive-bayes-classifier-a and\n",
    "https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/cross_validation.py\n",
    "\n",
    "Now, originally I looped the function as 100 runs -> C_list -> 10 fold, but this did not seem optimized for time becuase the SVM had to be generated 400 times. Some of the time came from generating SVMs. The time ran for 1 run successfully was around 10 seconds, a for 10 runs was around 5 minutes. Thus, we rearranged the loop to C_list -> 100 runs -> 10 fold, which only needed to generate 4 SVMs. However, we could not avoid the fit process for all 10 cross folds..so this did take a while. Now, we saw it kept decreasing downwards in error and each run ran for ~40 minutes, which didn't seem to entirely match the prior questions in educational guessing and in speed, so I ran with the cross_validation built in score function, which outputs the score based on classification, and in our case with the SVM, the score was how many of the data points it got correct (as a fractional value). This was great, because we wanted to find how many we didn't get correct, so the Error was a simple 1 - score. And since the function is optimized in sklearn, the speed was ~20 seconds, which is orders of magnitude faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "\n",
    "# parameters for kernel, SVM, now with cross validation\n",
    "C_list = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "Q_list = 2\n",
    "n_fold = 10\n",
    "\n",
    "# had to re-run previous programs, so just re-ran the 1 versus 5 dataset extraction for soundness.\n",
    "x_train_ = x_train[np.logical_or(y_train==1, y_train==5), :]\n",
    "y_t_b = bin_class(y_train[np.logical_or(y_train==1, y_train==5)], 1)\n",
    "\n",
    "C_E_in = []\n",
    "for C in C_list:\n",
    "    clf = svm.SVC(C=C, kernel='poly', degree=Q, gamma=1, coef0=1)\n",
    "    \n",
    "    C_i_E_in = []\n",
    "    for r in xrange(100):      \n",
    "        # split into 10 fold cross validation, stores indices of [train], [test]\n",
    "        k_fold = KFold(len(y_t_b), n_folds=10, shuffle=True)\n",
    "        \n",
    "        # take average of the 10 cv folds and compute E_in (function inherent in cross_validation)\n",
    "        # we note that the val_score varies based on classifier, for the SVM, it's a fraction of correct predictions\n",
    "        # we just do 1 - frac(correct) to yield the desired error\n",
    "        C_i_E_in.append(np.mean(1 - cross_val_score(clf, x_train_, y_t_b, cv=k_fold, n_jobs=1)))\n",
    "    \n",
    "    C_E_in.append(C_i_E_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C_E_in = np.array(C_E_in).T\n",
    "min_E_in = np.amin(C_E_in, axis=1) \n",
    "min_C = np.argmin(C_E_in, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 37, 2: 28, 3: 18, 4: 17})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# C_list values [0.0001, 0.001, 0.01, 0.1, 1] correspond to indices [0, 1, 2, 3, 4]\n",
    "print collections.Counter(min_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Answer Choice: **C**, 0.005<br>\n",
    "Reasoning: see code. The closest value in range is 0.005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0058023027927486398"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(C_E_in[np.where(min_C == 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "Answer Choice: **E**, $C = 10^6$ <br>\n",
    "Reasoning: see code. Now, SVC uses rbf (radial basis function) kernel instead of the polynomial kernel. Again, we still use the same one-versus-one condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_in for C = 0.010000:  0.00384368994234\n",
      "E_in for C = 1.000000:  0.00448430493274\n",
      "E_in for C = 100.000000:  0.00320307495195\n",
      "E_in for C = 10000.000000:  0.00256245996156\n",
      "E_in for C = 1000000.000000:  0.000640614990391\n"
     ]
    }
   ],
   "source": [
    "# parameters for kernel, SVM\n",
    "C_list = [0.01, 1, 100, 10**4, 10**6]\n",
    "Q_list = 2\n",
    "n_fold = 10\n",
    "\n",
    "for C in C_list:\n",
    "    clf = svm.SVC(C=C, kernel='rbf', gamma=1)\n",
    "    clf.fit(x_train_, y_t_b)\n",
    "    \n",
    "    y_t_b_ = clf.predict(x_train_)\n",
    "    \n",
    "    E_in = np.sum(abs(y_t_b_ + y_t_b) == 0) / float(len(y_t_b_))\n",
    "    print 'E_in for C = %f: ' %C, E_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "Answer Choice: **C**, 100<br>\n",
    "Reasoning: see code. It is interesting to note that for $C = 10^4, 10^6$ yields the same out of sample error. We run with higher $C$, and note that the error converges, which is what we expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_out for C = 0.010000:  0.0235849056604\n",
      "E_out for C = 1.000000:  0.0212264150943\n",
      "E_out for C = 100.000000:  0.0188679245283\n",
      "E_out for C = 10000.000000:  0.0235849056604\n",
      "E_out for C = 1000000.000000:  0.0235849056604\n"
     ]
    }
   ],
   "source": [
    "# parameters for kernel, SVM\n",
    "C_list = [0.01, 1, 100, 10**4, 10**6]\n",
    "Q_list = 2\n",
    "n_fold = 10\n",
    "\n",
    "for C in C_list:\n",
    "    clf = svm.SVC(C=C, kernel='rbf', gamma=1)\n",
    "    clf.fit(x_train_, y_t_b)\n",
    "    \n",
    "    y_t_b_ = clf.predict(x_test_)\n",
    "    \n",
    "    E_out = np.sum(abs(y_t_b_ + y_test_b) == 0) / float(len(y_t_b_))\n",
    "    print 'E_out for C = %f: ' %C, E_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_out for C = 10000.000000:  0.0235849056604\n",
      "E_out for C = 1000000.000000:  0.0235849056604\n",
      "E_out for C = 10000000.000000:  0.0259433962264\n",
      "E_out for C = 100000000.000000:  0.0259433962264\n",
      "E_out for C = 10000000000.000000:  0.0259433962264\n",
      "E_out for C = inf:  0.0259433962264\n"
     ]
    }
   ],
   "source": [
    "# parameters for kernel, SVM\n",
    "C_list = [10**4, 10**6, 10**7, 10**8, 10**10, np.inf]\n",
    "Q_list = 2\n",
    "n_fold = 10\n",
    "\n",
    "for C in C_list:\n",
    "    clf = svm.SVC(C=C, kernel='rbf', gamma=1)\n",
    "    clf.fit(x_train_, y_t_b)\n",
    "    \n",
    "    y_t_b_ = clf.predict(x_test_)\n",
    "    \n",
    "    E_out = np.sum(abs(y_t_b_ + y_test_b) == 0) / float(len(y_t_b_))\n",
    "    print 'E_out for C = %f: ' %C, E_out"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
