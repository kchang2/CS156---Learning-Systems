{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5\n",
    "## CS156, Kai Chang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Answer Choice: **C**, 100 <br>\n",
    "Reasoning: Using Mathematica, we want $E_in$ to be $> 0.008$, meaning that we want the minimal # of examples ($N$) given in the answer choices that accurately reflect a value $> 0.008$. So, plugging into Mathematica the following \n",
    "\n",
    "+ Solve[0.008 == 0.1^2*(1 - (8 + 1)/x), x]\n",
    "\n",
    "We arrive that $x$, or in our case $x = N$ is exactly 45, meaning that we need the next value larger than 45 to get an error greater than 0.008. Thus, C is the correct choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Answer Choice: **D**, $\\tilde w_1 < 0, \\tilde w_2 > 0$ <br>\n",
    "Reasoning: Originally, I thought about the fact that when we transform these two values by squring them, we result in a positive value across all scales. This led me to believe that the weights are all bound by positivity. However, this would never allow me to correctly classify my points. In order for this hyperbolic problem to work, we would need to have the two weights corresponding to both $x_1^2$ and $x_2^2$ be of opposite signs such that we could potentially classify based on these boundaries. \n",
    "\n",
    "If we have our weight on the $x_1$ be negative then for all large values of $x_1$ in magnitude (because we are squaring), we will more likely get a domination of the negative weight, giving us a -1 hypothesis (sign function). If we have a positive weight on $x_2$, then we would more likely get a positive domination in the classification from our transformation, and result in a +1.\n",
    "\n",
    "Thus, this is why D is the only valid choice. Note any magnitude change or shift can be adjusted by the $w_0$ term, and any slope change in the hyperbolic decision boundary can be adjusted by the ratio of $w_1$ and $w_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Answer Choice: **C** <br>\n",
    "Reasoning: From the *Learning from Data* book, we know that linear regression is often coupled with a feature transform to perform nonlinear regression, and that the $N$ by $d+1$ input matrix X in the algoirthm is replaced with the $N$ by a $\\tilde d + 1$ matrix $Z$, while the ouput vector remians the same. The VC dimensionality increase ($d_{vc}$) is still $n+1$, so in an n-dimensional polynomial ($Z$ degree n), it's simply $n+1$, that $w_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Answer Choice: **E**, $2 \\left ( e^v + 2ve^{-u} \\right )\\left( ue^v - 2ve^{-u} \\right)$ <br>\n",
    "Reasoning: Partial derivatives suggest derivatives without consideration of dependent variables. Thus, the partial derivative is as follows:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial E}{\\partial u} &= \\frac{\\partial \\left ( ue^v - 2ve^{-u} \\right )^2}{\\partial u} \\\\\n",
    "    &= 2 \\left( ue^v - 2ve^{-u} \\right) * \\frac{\\partial \\left ( ue^v - 2ve^{-u} \\right )}{\\partial u} \\\\\n",
    "    &= 2 \\left( ue^v - 2ve^{-u} \\right) \\left ( e^v - 2ve^{-u}*\\frac{-\\partial u}{\\partial u} \\right) \\\\\n",
    "    &= 2 \\left( ue^v - 2ve^{-u} \\right) \\left ( e^v + 2ve^{-u} \\right )\n",
    "\\end{align}\n",
    "\n",
    "which matches with answer choice e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Answer Choice: **D**, 10<br>\n",
    "Reasoning: See code. We know that gradient descent is given by $(u_{t+1}, v_{t+1}) = (u_t, v_t) - \\eta \\triangledown(u,v)$ (originally I had written $+\\eta$, but that gave me a value not given in the item choice and I realized it was wrong. So we just calculate the gradient as by Mathematica:\n",
    "\n",
    "+ Grad[(u*Exp[v] - 2*v*Exp[-u])^2, {u, v}]\n",
    "\n",
    "and then run the iterations, and stop when the error gets is wihtin range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def E_func(u, v):\n",
    "    return (u*np.exp(v) - 2*v*np.exp(-u))**2\n",
    "\n",
    "def update(u, v, eta):\n",
    "    d_u = lambda u,v: 2*(np.exp(v)*u - 2*np.exp(-u)*v)*(np.exp(v) + 2*np.exp(-u)*v)\n",
    "    d_v = lambda u,v: 2*(-2*np.exp(-u) + np.exp(v)*u)*(np.exp(v)*u - 2*np.exp(-u)*v)\n",
    "    \n",
    "    return u - eta * d_u(u,v) , v - eta * d_v(u,v)\n",
    "    \n",
    "u = 1\n",
    "v = 1\n",
    "eta = 0.1\n",
    "\n",
    "\n",
    "for i in xrange(17):\n",
    "    if E_func(u, v) > 10**(-14):\n",
    "        u, v = update(u, v, eta)\n",
    "    else:\n",
    "        print i\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Answer Choice: **E**, (0.045, 0.024)<br>\n",
    "Reasoning: See code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0447362903978 0.0239587140991\n"
     ]
    }
   ],
   "source": [
    "print u,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "Answer Choice: **A**, $10^{-1}$<br>\n",
    "Reasoning: see code. The point of this question is to show that we must update our parameters at the same time, otherwise, it will take exponentially longer to get to an appropriate minima or maxima. This has to do with the fact that although the gradients are independent of each other (change), with the physical gradient descent update rule, they are strictly tied to each other when deciding the next step in u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.139813791996\n"
     ]
    }
   ],
   "source": [
    "def coord_update(u, v, eta):\n",
    "    d_u = lambda u,v: 2*(np.exp(v)*u - 2*np.exp(-u)*v)*(np.exp(v) + 2*np.exp(-u)*v)\n",
    "    u = u - eta * d_u(u,v)\n",
    "    d_v = lambda u,v: 2*(-2*np.exp(-u) + np.exp(v)*u)*(np.exp(v)*u - 2*np.exp(-u)*v)\n",
    "    v = v - eta * d_v(u,v)\n",
    "    \n",
    "    return u, v\n",
    "\n",
    "u = 1\n",
    "v = 1\n",
    "eta = 0.1\n",
    "\n",
    "for i in xrange(15):\n",
    "    u, v = coord_update(u, v, eta)\n",
    "    \n",
    "print E_func(u, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Answer Choice: **D**, 0.100<br>\n",
    "Reasoning: see code.\n",
    "\n",
    "Remember, random permutation means that we have to go through all the training data for a single epoch -- ie. we simple shuffle the order in which we analyze our gradient descent. A more intuitive way to think about it is picking which data point to analyze or update with without putting that point back into the list we pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 100)\n",
      "(4, 5000)\n",
      "(4, 100)\n",
      "(4, 5000)\n",
      "(4, 100)\n",
      "(4, 5000)\n",
      "(4, 100)\n",
      "(4, 5000)\n",
      "(4, 100)\n",
      "(4, 5000)\n",
      "(4, 100)\n",
      "(4, 5000)\n",
      "(4, 100)\n",
      "(4, 5000)\n",
      "(4, 100)\n",
      "(4, 5000)\n",
      "(4, 100)\n",
      "(4, 5000)\n",
      "(4, 100)\n",
      "(4, 5000)\n",
      "(4, 100)\n",
      "(4, 5000)\n",
      "(4, 100)\n",
      "(4, 5000)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-397e954e28b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_prior\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;31m# continuation of next epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mw_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kaichang/anaconda/lib/python2.7/site-packages/numpy/linalg/linalg.pyc\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m     \"\"\"\n\u001b[0;32m-> 2113\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random as rnd\n",
    "\n",
    "def gen_line():\n",
    "    '''\n",
    "    Generate boundary line for classification\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    2 2-dimensional array consisting of your line in form [w0, w1, w2] and [w0, w1_norm, w2_norm]\n",
    "    \n",
    "    '''\n",
    "    # apparently from [-1.0, 1.0], but upper boundary not guaranteed\n",
    "    # because of float's precision and rounding issues\n",
    "    [x1,x2,y1,y2] = [rnd.uniform(-1.0, 1.0), rnd.uniform(-1.0, 1.0), rnd.uniform(-1.0, 1.0), rnd.uniform(-1.0, 1.0)] \n",
    "    w = np.array([x2*y1-y2*x1, y2-y1, x1-x2])\n",
    "#     w_norm = np.array([1, -w[1]/w[2], -w[0]/w[2]]) #standard mx+b equation\n",
    "    return w, [x1,x2,y1,y2]\n",
    "    \n",
    "def gen_pts(n, d, w=None):\n",
    "    '''\n",
    "    Generates random points from a uniform distribution over -1,1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : number of points\n",
    "    d : dimension of image\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    d-dimensional array consisting of n-number of uniform, random points, and a clean slate sign\n",
    "    '''\n",
    "    if w is None:\n",
    "        w, li = gen_line()\n",
    "        \n",
    "    d_ = np.random.uniform(-1.0, 1.0,(d,n))\n",
    "    x_ = np.append(np.ones(n), d_).reshape((d+1,n))\n",
    "    y = np.sign(np.dot(w.T,x_))\n",
    "    d_ = np.append(x_, y).reshape((d+2,n))\n",
    "\n",
    "    print d_.shape\n",
    "    return w, d_\n",
    "\n",
    "def compute_gradient(w, x_n, y_n):\n",
    "    '''\n",
    "    Computes the gradient for SGD\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w    : current weights    \n",
    "    x_n  : training point information (1, x1, x2)\n",
    "    y_n  : solution to training point\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    gradient needed for the weight update (v_t)\n",
    "    '''\n",
    "    return -y_n*x_n/(1+np.exp(y_n*np.dot(w.T,x_n)))\n",
    "    \n",
    "def update(w, d_, eta, rand_perm):\n",
    "    '''\n",
    "    iterates to the next step\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w         : weights\n",
    "    d         : train set\n",
    "    eta       : learning rate\n",
    "    rand_perm : order to update in\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    new weights\n",
    "    '''\n",
    "    for n in rand_perm:\n",
    "        x_n = np.array([d_[0][n], d_[1][n], d_[2][n]])\n",
    "        y_n = np.array([d_[3][n]])\n",
    "    \n",
    "        v_t = -compute_gradient(w, x_n, y_n)\n",
    "        w = w + eta * v_t\n",
    "    \n",
    "    return w\n",
    "\n",
    "def calc_error(w, d_):\n",
    "    '''\n",
    "    Calculates the E_out (error of out sample)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w  : weights learned from SGD\n",
    "    d_ : test set\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    E_out\n",
    "    '''\n",
    "    \n",
    "    return np.sum(np.log((1+np.exp(-d_[3]*np.dot(w.T,d_[0:3])))))/ len(d_[0])\n",
    "\n",
    "\n",
    "\n",
    "# specify eta\n",
    "eta = 0.01\n",
    "\n",
    "#E_out, Epoch\n",
    "E_out_list = []\n",
    "epoch_list = []\n",
    "\n",
    "for i in xrange(100):\n",
    "    # generate training set\n",
    "    w, d_ = gen_pts(100,2)\n",
    "\n",
    "    # test set\n",
    "    _, d_test = gen_pts(5000, 2, w=w)\n",
    "\n",
    "    # initialize weight\n",
    "    w_init = np.array([0.0, 0.0, 0.0])\n",
    "    w_prior = w_init\n",
    "    \n",
    "    # stochastic generation of order for SGD\n",
    "    rand_perm = np.random.permutation(len(d_[0]))\n",
    "    \n",
    "    # 1 epoch\n",
    "    w_ = update(w_prior, d_, eta, rand_perm)\n",
    "    epoch = 1\n",
    "    \n",
    "    while np.linalg.norm(w_prior - w_) >= 0.01:\n",
    "        # continuation of next epoch\n",
    "        w_prior = w_\n",
    "        rand_perm = np.random.permutation(len(d_[0]))\n",
    "        w_ = update(w_prior, d_, eta, rand_perm)\n",
    "        epoch += 1\n",
    "        \n",
    "    E_out_list.append(calc_error(w_, d_test))\n",
    "    epoch_list.append(epoch)\n",
    "\n",
    "print 'E_out: ', np.sum(E_out_list)/len(E_out_list)\n",
    "print 'epoch: ', np.sum(epoch_list)/len(epoch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x10920c110>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1091c9450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# shows that the normalized weights are useful for direct plotting\n",
    "# rather than using the two points to form a line\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# x = np.arange(-1,1, 0.1)\n",
    "# y = w_norm[1]*x + w_norm[2]\n",
    "# plt.plot(x,y, color='k', linestyle='-', linewidth=2)\n",
    "# # plt.plot([li[0], li[1]],[li[2],li[3]], color='k', linestyle='-', linewidth=2)\n",
    "\n",
    "# pos_x = []\n",
    "# neg_x = []\n",
    "# pos_x2 = []\n",
    "# neg_x2 = []\n",
    "# for i in xrange(len(d_[0])):\n",
    "#     if d_[3][i] == 1:\n",
    "#         pos_x.append(d_[1][i])\n",
    "#         pos_x2.append(d_[2][i])\n",
    "#     else:\n",
    "#         neg_x.append(d_[1][i])\n",
    "#         neg_x2.append(d_[2][i])\n",
    "        \n",
    "# plt.scatter(pos_x, pos_x2)\n",
    "# plt.scatter(neg_x, neg_x2, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "Answer Choice: **A**, 350 <br>\n",
    "Reasoning: see above code. Remember, our initialization is all 0s for our weights, and termination is when the magnitude of the difference between the current step and the previous step weights is $< 0.01$, and the learning rate is $\\eta = 0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "Answer Choice: **E**, $e_n(w) = - \\text{min}(0, y_n w^Tx_n)$ <br>\n",
    "Reasoning: SGD is similar to PLA in that it decreses the error with respect to one data point at a time. Now, looking at the example in 3.3 from the book (Exercise 3.10), we simply find any SGD error function such that it makes no descent when the hypothesis ($w^Tx_n$) of the data point matches the solution ($y_n$) of that data point, and then updates when the hypothesis does NOT match the solution. We also look for errors which don't update (0) for the hypothesis matches the solution.\n",
    "\n",
    "Thus it spans (analyzes each) all data points and adjusts for the ones that the error is wrong (repeating with the entire data set as new after each adjustment). \n",
    "\n",
    "Thus, we know that a, b, d is incorrect because it will always adjust no matter what (we see a is always positive, if correct 2x positive).\n",
    "\n",
    "So, between c and e, we note that c will spiral out of control because it will always adjust positively, unless we start with the hypothesis matching the solution. \n",
    "\n",
    "Thus, the answer must be e. This makes sense because when the hypothesis ($w^Tx_n$) is NOT of the same classification or sign to the solution ($y_n$), the value must be opposite of each other (ie. a negative value), and so the error will choose this over 0 (because minimum), and then convert it to a positive value. However, when they ARE of the same sign, then both hypothesis and solution are either positive or negative, and will result in a positive value, leaving our error to correctly choose 0 as the minimum (meaning we correctly classified the point and will not update for this point), and -0 is still 0."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
