{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1, CS156a\n",
    "##### Kai Chang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Answer choice: **D** -- (i) Not Learning, (ii) Supervised Learning, (iii) Reinforcement Learning\n",
    "\n",
    "Reasoning:<br>\n",
    "For choice (i), this is not considered Machine Learning because you are taking known parameters or feature values of the coins and matching them statistically with your sample. In this case, your necessary or classification features have been already set, not found.\n",
    "\n",
    "For choice (ii), this is machine learning because in this case, you don't know the correct classification features or exact values needed to classify the coins into the appropriate groupings. You have samples in which you are trying to infer the correct grouping by the feature information given from your large set of labeled data. It is supervised because you know the correct classification or answer.\n",
    "\n",
    "For choice (iii), this is machine learning because again, you don't know the exact approach or process to obtain a winning strategy -- you are essentially learning by trial and error. This is reinforced because you adjust your learning over time with respect to your successes (in this case Validation cost = win %). Reinforcement elarning changes your $\\alpha$ such that your *w* and *b* adjust approriately over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Answer choice: **A** -- (ii) and (iv)\n",
    "\n",
    "Reasoning:<br>\n",
    "For choice (i), there is already an solution for it without using ML, ie. it is mathematically solvable, so machine learning would not offer any benefits. Thus, we rule out choice (i).\n",
    "\n",
    "Choice (iii) is also exact and has a fixed solution within realms, and like choice (i), would not be effective for ML usage.\n",
    "\n",
    "(ii) and (iv) are the best problems suited for ML because there are no relatively easy exact formulas to solve our problem and would take a lot of time, so ML is best suited to learn the necessary features. In this case, we don't necessary know how to definie our classification parameters and which parameters are significant or nonessential in solving our problems, so we feed in as much information as we can and allow our computer to classifiy and form appropriate groupings of our similar traits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Answer choice: **D** -- 2/3\n",
    "\n",
    "Originally, I had thought using conditional probability that because 1 bag is $\\frac{1}{2}$ (b,b) and the other is $\\frac{1}{2}$ (b,w), that $P = \\frac{1}{2}$ because once you pick a bag $\\frac{1}{2}$, then you automatically know if you win or lose. However, I completely neglected the conditional aspect, ie. the first is black (has to be black). This rules out any of the probability that the first ball is white, which would alter our probability (total outcome pool).\n",
    "\n",
    "My first thoughts (because I needed a refresher on stats) is to run this scenario through a python simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of second ball black:  0.667124\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "# Parameters\n",
    "bag1 = ['b','b']\n",
    "bag2 = ['b','w']\n",
    "\n",
    "# Question 3\n",
    "def pick_ball(bag):\n",
    "    return random.choice(bag)\n",
    "\n",
    "def pick_bag(bag_set):\n",
    "    return random.choice(bag_set)\n",
    "\n",
    "def select(bag_set):\n",
    "    bag = pick_bag(bag_set)\n",
    "    return bag, pick_ball(bag)\n",
    "\n",
    "b_black = 0 # counts of black ball being picked after 1st black ie. P(A|B)\n",
    "n = 1000000 # total trials in simulation\n",
    "for i in xrange(n):\n",
    "    b1 = copy.deepcopy(bag1)\n",
    "    b2 = copy.deepcopy(bag2)\n",
    "    b_set = [b1,b2]\n",
    "    ball = 'w'\n",
    "\n",
    "    while ball != 'b':\n",
    "        bag, ball = select(b_set)\n",
    "\n",
    "    # get first bag\n",
    "    bag.remove('b')\n",
    "    if bag[0] == 'b':\n",
    "        b_black += 1\n",
    "\n",
    "print 'Probability of second ball black: ', float(b_black)/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, using 1 million simulations, we get $P \\approx \\frac{2}{3}$. However after thinking mathematically, this was a clear useage of Baye's theorem.\n",
    "\n",
    "\\begin{equation}\n",
    "    P(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n",
    "\\end{equation}\n",
    "and\n",
    "\n",
    "+ $P(A|B)$, probability we choose bag (b,b), given the first ball we chose is b\n",
    "+ $P(B|A) = 1$, probability the first ball is b, given we chose bag (b,b)\n",
    "+ $P(A) = \\frac{1}{2}$, probability we choose bag (b,b)\n",
    "+ $P(B) = \\frac{3}{4}$, probability that the first ball is b\n",
    "\n",
    "So, we get \n",
    "\n",
    "\\begin{align}\n",
    "    P(A|B) &= \\frac{1 \\times \\frac{1}{2}}{\\frac{3}{4}} \\\\\n",
    "            &= \\frac{2}{3}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Answer choice: **B**, $3.405 \\times 10^{-4}$\n",
    "\n",
    "Given 10 marbles, we draw one sample. The chance that $\\nu = 0$ or we get 0 chance of red marbles has is such that every marble in the pot is green. Thus, because $\\mu = 0.55$, or a chance a marble is red (independent and with replacement!), then we calculate P for 10 marble cases.\n",
    "\n",
    "So, $P = (1-0.55) = 0.45$ for green pull on a single marble, and $P = 0.45^{10} = 0.003405$ for green pulls always on 10 marbles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Answer choice: **C**, 0.289\n",
    "\n",
    "Now, given that for a 1000 *independent* samples, we pull at least 1 $\\nu=0$ means that we can have at 1 sample, 2 samples, 3 samples all the way up to all the samples having $\\nu=0$. This is equivalent to taking the probability of $1 - P(\\text{we get all red marbles for }1000 \\text{ samples})$. \n",
    "\n",
    "This is equal to $1-P(0 \\text{ } \\nu \\text{ or all } \\mu)$, which is $1 - (1- P(\\text{all red in a single sample}))^{1000}$, and this is $1 - (1 - 0.0003403)^{1000} = 0.289$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Answer choice: **E**, They are all equivalent (equals cores for *g* in a through d).\n",
    "\n",
    "Reasoning: For (a), following the 2D example on the edX forum, using g = 1, score for the three remaining points are\n",
    "\n",
    "\\begin{equation}\n",
    "    1 \\times 3 + 3 \\times 2 + 3 \\times 1 + 1 \\times 0 = 12\n",
    "\\end{equation}\n",
    "For (b), g = 0, score is\n",
    "\\begin{equation}\n",
    "    1 \\times 3 + 3 \\times 2 + 3 \\times 1 + 1 \\times 0 = 12\n",
    "\\end{equation}\n",
    "\n",
    "These two are identical, meaning the only valid solution is the last one.\n",
    "\n",
    "We note that the possible $y_n$ outcomes or target functions for all cases are pallandromic and follow combinatoric patterns, where they contain the same unique 3 point identicals, 2 point identicals, and 1 point identicals. For a and b, these are identical but inverse in values, and for c and d, this is the same. So, if it works on one case, it must work on the other. Thus, we realize that **E** must be the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "Answer choice: **B**, 15\n",
    "\n",
    "See attached code and afterwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def gen_line():\n",
    "    '''\n",
    "    Generate boundary line for classification\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    2 2-dimensional array consisting of your line in form [w0, w1, w2] and [w0, w1_norm, w2_norm]\n",
    "    \n",
    "    '''\n",
    "    [x1,x2,y1,y2] = [rnd.uniform(-1.0, 1.0), rnd.uniform(-1.0, 1.0), rnd.uniform(-1.0, 1.0), rnd.uniform(-1.0, 1.0)]\n",
    "    xA,yA,xB,yB = [rnd.uniform(-1, 1) for i in range(4)]\n",
    "    w = np.array([x2*y1-y2*x1, y2-y1, x1-x2])\n",
    "    w_norm = np.array([1, -w[1]/w[2], -w[0]/w[2]])\n",
    "    return w, w_norm\n",
    "    \n",
    "def gen_pts(n, d, w=None, w_norm=None):\n",
    "    '''\n",
    "    Generates random points from a uniform distribution over -1,1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : number of points\n",
    "    d : dimension of image\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    d-dimensional array consisting of n-number of uniform, random points, and a clean slate sign\n",
    "    '''\n",
    "    if w is None:\n",
    "        w, w_norm = gen_line()\n",
    "        \n",
    "    d_ = np.random.uniform(-1.0, 1.0,(d,n))\n",
    "    x_ = np.append(np.ones(n), d_).reshape((d+1,n))\n",
    "    y = np.sign(np.dot(w.T,x_))\n",
    "    d_ = np.append(x_, y).reshape((d+2,n))\n",
    "    return x_, y, w, d_, w_norm\n",
    "\n",
    "def pick_pt(y_, y):\n",
    "    '''\n",
    "    Find misclassified points and pick one at random.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_ : list of all output points from our updated weight\n",
    "    y  : list of correct output points\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    index of random point, number of misclassified points\n",
    "    '''\n",
    "    mc_pts = []\n",
    "    for i in xrange(len(y)):\n",
    "        if y_[i] != y[i]:\n",
    "            mc_pts.append(i)\n",
    "    \n",
    "    try:\n",
    "        index = rnd.choice(mc_pts)\n",
    "    except IndexError:\n",
    "        index = 0\n",
    "    \n",
    "    return index, len(mc_pts)\n",
    "    \n",
    "\n",
    "def update(xi, yi_, w_):\n",
    "    '''\n",
    "    Takes a misclassified point and updates the weight to correctly classify point\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    xi   : incorrectly classified point\n",
    "    yi_  : correct sign for point\n",
    "    w_   : current weight\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    updated weight\n",
    "    '''\n",
    "    w_ += yi_ * xi\n",
    "\n",
    "    return w_\n",
    "    \n",
    "def pre_process(n, d):\n",
    "    '''\n",
    "    Creates the necessary datasets and solutions needed to run a PLA classification\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : number of data points\n",
    "    d : dimensions of dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x_  : coordinates or feature information (1, x1, x2)\n",
    "    y   : solution from sign function\n",
    "    w   : true weights (w0, w1, w2)\n",
    "    d_  : entire dataset (incl. solution)\n",
    "    w_n : normalized weights, ie. (w0=1, w1, w2)\n",
    "    '''\n",
    "    x_, y, w, d_, w_n = gen_pts(n,d)\n",
    "    \n",
    "    return x_, y, w, d_, w_n\n",
    "\n",
    "#     xp = d_[:-1,d_[-1]>0]\n",
    "#     xm = d_[:-1,d_[-1]<0]\n",
    "\n",
    "#     fig = plt.figure(figsize=(6, 6))\n",
    "#     plt.plot( xp[1], xp[2], 'bo')\n",
    "#     plt.plot( xm[1], xm[2], 'ro' )\n",
    "#     x = np.linspace(-1,1)\n",
    "#     plt.plot(x, w_n[1]*x + w_n[2], color='black')\n",
    "#     plt.title('PLA Classification')\n",
    "#     plt.ylim([-1,1])\n",
    "\n",
    "\n",
    "def pla():\n",
    "    w_ = np.zeros(3)\n",
    "    y_ = np.sign(np.dot(w_.T,x_))\n",
    "    i = 0 # iterations\n",
    "\n",
    "    while np.array_equal(y, y_) != True:\n",
    "        index, total_mc_pts= pick_pt(y_,y)\n",
    "    #     print 'i:', index, ' y_:', y_[index], ' y:', y[index]\n",
    "#         print '# of misclassified points after iter %i: %i' %(i, total_mc_pts)\n",
    "        w_ = update(x_[:,index], y[index], w_)\n",
    "        y_ = np.sign(np.dot(w_.T, x_))\n",
    "        i += 1\n",
    "#         plt.plot(x, w_[1]*x + w_[2], color='g', alpha=0.1)\n",
    "        if i%1000 == 0:\n",
    "            break\n",
    "    w_n = np.array([1, -w_[1]/w_[2], -w_[0]/w_[2]])\n",
    "#     print 'updated:', y_, ' sol:', y\n",
    "#     print w\n",
    "#     print w_n\n",
    "\n",
    "#     plt.plot(x, w_[1]*x + w_[2], color='g')\n",
    "    return i, w_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations away from 15: 5.259  iterations away from 1: 8.741\n"
     ]
    }
   ],
   "source": [
    "# Question 7\n",
    "iters = []\n",
    "n = 10\n",
    "d = 2\n",
    "for i in xrange(1000):\n",
    "    x_, y, w, d_, w_n = pre_process(n, d)\n",
    "    it, _ = pla()\n",
    "    iters.append(it)\n",
    "    \n",
    "avg_iter = sum(iters) / float(len(iters))\n",
    "print 'iterations away from 15:', 15-avg_iter, ' iterations away from 1:', abs(1-avg_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Afternotes: \n",
    "I originally struggled for hours reasoning why my linear line was not correctly plotting with my PLA Classification values. My weights had 2-dimensions {w0, w1}, and either resulted in a linear shift or a transpose shift. They were never constant and it threw me off.\n",
    "\n",
    "Eventually I remembered about the threshold term on the weight, in which Andrew Ng corresponded it to what usually people would put as 1. This made our weight 3-D in {w0, w1, w2}. This I think correspondes to the additional fact that we cannot just absorb the w2 term into our w0 and w1 term from w0 + w1x + w2y = 0. This artificial term is needed and is often overstepped by convention. Without it, we couldn't have a true intercept term or bias term, and with a transpose would really change our perception of our data. \n",
    "\n",
    "If you have a better or (correct) explanation, please do share.\n",
    "\n",
    "Note we know the exact solution, so this case we don't need to have a validation error and stop the program short if it fluctuates. We simply just keep processing until we get 100% accuracy.\n",
    "\n",
    "\n",
    "#### Justification for weight and normalized weight\n",
    "We use $w = (w_0, w_1, w_2) = (x_2y_1-x_1y_2, \\ y_2-y_1, \\ x_2-x_1)$. Why?\n",
    "\n",
    "Consider the generalized 2D linear equation to be $ w_0 + w_1x + w_2y = 0$, and we have two points $(x_1, y_1)$, $(x_2, y_2)$.\n",
    "\n",
    "Then, using the linear equation in two-point form (https://en.wikipedia.org/wiki/Linear_equation#Two-point_form), we can derive $y - y_1 = \\frac{y_2-y_1}{x_2-x_1}(x-x_1)$.\n",
    "\n",
    "This can be re-written as $(x_2-x_1)(y-y_1) = (y_2 - y_1) (x-x_1)$ and can be expanded to the form $x_2y_1 - x_1y_2 + (y_2-y_1) x - (x_2 - x_1) y = 0$.\n",
    "\n",
    "So, we then get \n",
    "+ $w_0 = x_2y_1 - y_2x_1$\n",
    "+ $w_1 = y_2-y_1$\n",
    "+ $w_2 = x_1-x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Answer choice: **C**, 0.1\n",
    "\n",
    "Reasoning: See code. Also, I put in 1 training point too, and got P~0.5, but I dont think this is correct because we have to keep the same sample size per trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of disagreement for N=10:  0.0963\n"
     ]
    }
   ],
   "source": [
    "disagreement = []\n",
    "n = 10\n",
    "d = 2\n",
    "for i in xrange(1000):\n",
    "    x_, y, w, d_, w_n = pre_process(n, d)\n",
    "    _, w_n_ = pla()\n",
    "    x_, y, _, _, _ = gen_pts(10, d, w, w_n) # 10 training points (ie so we have 10000 samples from 10 point * 1000 trials)\n",
    "    y_ = np.sign(np.dot(w_n_.T,x_))\n",
    "    zzz, nmc = pick_pt(y_, y)\n",
    "    if nmc > 0:\n",
    "        disagreement.append(nmc)\n",
    "    \n",
    "print 'probability of disagreement for N=10: ', len(disagreement)/float(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "Answer choice: **B**, 100\n",
    "\n",
    "Reasoning: see code. It shouldn't ever really be larger than the total trials for any usable algorithm however bad, so it ruled out 1000 and 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations away from 50: 46.941  iterations away from 100: 3.059  iterations away from 500:  403.059\n"
     ]
    }
   ],
   "source": [
    "iters = []\n",
    "n = 100\n",
    "d = 2\n",
    "for i in xrange(1000):\n",
    "    x_, y, w, d_, w_n = pre_process(n, d)\n",
    "    it, _ = pla()\n",
    "    iters.append(it)\n",
    "\n",
    "avg_iter = sum(iters) / float(len(iters))\n",
    "print 'iterations away from 50:', abs(50-avg_iter), ' iterations away from 100:', abs(100-avg_iter), ' iterations away from 500: ', abs(500-avg_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "Answer choice: **B**, 0.01\n",
    "\n",
    "Reasoning: see code. It should be less than the 10 training points set because with more data there is more information and thus less chance for validation errors. More sample statistics almost always lead to a better resolution. This rules out (d), (e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of disagreement for N=10:  0.00999\n"
     ]
    }
   ],
   "source": [
    "disagreement = []\n",
    "n = 100\n",
    "d = 2\n",
    "for i in xrange(1000):\n",
    "    x_, y, w, d_, w_n = pre_process(n, d)\n",
    "    _, w_n_ = pla()\n",
    "    x_, y, _, _, _ = gen_pts(100, d, w, w_n) # 10 training points (ie so we have 10000 samples from 10 point * 1000 trials)\n",
    "    y_ = np.sign(np.dot(w_n_.T,x_))\n",
    "    zzz, nmc = pick_pt(y_, y)\n",
    "    if nmc > 0:\n",
    "        disagreement.append(nmc)\n",
    "    \n",
    "print 'probability of disagreement for N=10: ', len(disagreement)/float(100000)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
